# TDD и BDD для курса «Инженерия Искусственного Интеллекта»

Этот файл — краткая инженерная заметка:

- чем отличаются TDD и BDD;
- как они помогают в ИИ-проектах (данные, пайплайны, сервис);
- простые примеры под Python/FastAPI/ML.

---

## 1. Зачем TDD/BDD в курсе

В ИИ-проектах легко уйти в «магические ноутбуки» и ручной дебаг.  
TDD и BDD помогают:

- не ломать базовую функциональность при изменениях;
- не превращать проект в нечитаемый набор экспериментов;
- держать сервис предсказуемым: одинаковые входы → ожидаемые выходы;
- документировать поведение через тесты (особенно API).

Важный момент: **мы не тестируем «рандомный» результат тренировки**, мы тестируем:

- корректность данных и предобработки;
- контракты функций и API;
- стабильность пайплайна;
- инварианты и грубые ошибки моделей.

---

## 2. TDD: Test-Driven Development

### 2.1. Идея

Классический цикл TDD:

1. **Red** — пишем маленький тест, который пока падает.
2. **Green** — пишем минимальную реализацию, чтобы тест прошёл.
3. **Refactor** — улучшаем код, сохраняя зелёные тесты.

Для курса достаточно помнить:

> «Сначала формулирую, что должно работать (тест), потом пишу код под это, а не наоборот».

### 2.2. Где TDD уместен в ИИ-проекте

- функции предобработки данных (`clean_...`, `encode_...`);
- вспомогательные функции и классы (`split_train_val`, `load_config`, `build_features`);
- сервисный код (`/predict`, `/health`, валидация входа и т.п.);
- небольшие куски бизнес-логики вокруг модели.

Пример: прежде чем писать функцию, которая нормализует числовые признаки, задаём ожидаемое поведение тестом.

---

## 3. Простой пример TDD под Python

### 3.1. Тест (Red)

`tests/test_preprocess.py`:

```python
from project.src.data.preprocess import scale_feature

def test_scale_feature_zero_mean_unit_std():
    values = [1.0, 2.0, 3.0]
    scaled = scale_feature(values)

    # среднее близко к 0
    mean = sum(scaled) / len(scaled)
    assert abs(mean) < 1e-6

    # дисперсия близка к 1
    var = sum((x - mean) ** 2 for x in scaled) / len(scaled)
    assert abs(var - 1.0) < 1e-6
````

Сначала тест упадёт: функции `scale_feature` ещё нет или она ничего не делает.

### 3.2. Реализация (Green)

`project/src/data/preprocess.py`:

```python
from typing import List

def scale_feature(values: List[float]) -> List[float]:
    n = len(values)
    if n == 0:
        return []

    mean = sum(values) / n
    centered = [x - mean for x in values]

    var = sum(x * x for x in centered) / n
    if var == 0:
        return [0.0 for _ in centered]

    std = var ** 0.5
    return [x / std for x in centered]
```

Дальше можно рефакторить, добавлять обработку `None`, numpy и т.п., не ломая тест.

---

## 4. Что тестировать в ML/ИИ-коде с помощью TDD

**Хорошие кандидаты:**

- Предобработку:

  - обработка пропусков;
  - кодирование категорий;
  - нормализация, бинаризация, разбиение по времени.
- Логику пайплайна:

  - корректное разделение train/val/test;
  - отсутствие утечек будущего (future leakage).
- API:

  - схема входных данных;
  - коды ошибок;
  - формат ответа;
  - edge-кейсы (пустые списки, некорректные типы).

**Что не нужно жёстко зашивать в тесты:**

- точное значение метрики (AUC, accuracy, MSE) до пятого знака;
- конкретные веса модели.

Можно проверять **минимальные ожидания**:

- нет падений на валидных входах;
- метрика не «совсем ужас» (например, не хуже случайного baseline).

---

## 5. BDD: Behaviour-Driven Development

### 5.1. Идея

BDD фокусируется на **поведении системы глазами пользователя/бизнеса**:

- описываем сценарии на понятном языке;
- «Given–When–Then» (Дано–Когда–Тогда);
- тесты становятся живой документацией.

BDD удобно применять к:

- API сервисов (в т.ч. ИИ-сервисов);
- интеграции модели в бизнес-процесс.

### 5.2. Шаблон Given–When–Then

На человеческом языке:

> **Дано** определённое начальное состояние
> **Когда** мы выполняем некоторое действие
> **Тогда** получаем ожидаемый результат

Пример сценария для сервиса предсказания:

> **Дано**, что сервис запущен и модель загружена
> **Когда** я отправляю валидный запрос с признаками пользователя
> **Тогда** я получаю ответ со статусом `200` и числовым полем `score` в диапазоне [0, 1]

---

## 6. BDD для API вокруг модели (пример)

### 6.1. Текст сценария

```text
Сценарий: Успешный запрос к /predict с валидными данными

  Дано запущенный сервис ИИ с загруженной моделью
  Когда я отправляю POST-запрос на /predict с корректным JSON:
    {
      "features": [0.1, 0.5, 1.2]
    }
  Тогда я получаю ответ:
    - HTTP статус 200
    - JSON с полем "score"
    - score — число от 0 до 1
```

### 6.2. Тест на Python (pytest + FastAPI TestClient)

```python
from fastapi.testclient import TestClient
from project.src.service.api import app

client = TestClient(app)

def test_predict_success():
    payload = {"features": [0.1, 0.5, 1.2]}
    response = client.post("/predict", json=payload)

    assert response.status_code == 200
    data = response.json()
    assert "score" in data
    assert isinstance(data["score"], (int, float))
    assert 0.0 <= data["score"] <= 1.0
```

Это уже BDD-подход в духе «черного ящика»: нас интересует поведение сервисного контракта, а не детали реализации.

---

## 7. Примеры BDD-сценариев для курса

Идеи сценариев, которые можно описать и покрыть тестами:

1. **Валидация входа**

   - Дано: запрос без обязательного поля
   - Когда: вызывается `/predict`
   - Тогда: ответ 422 или 400 и понятное сообщение об ошибке

2. **Обработка пустых/аномальных данных**

   - Дано: входная выборка пустая
   - Когда: вызывается endpoint batch-предсказаний
   - Тогда: сервис возвращает корректную ошибку или пустой результат, а не падает с 500

3. **Стабильность контракта при обновлении модели**

   - Дано: новая версия модели
   - Когда: старый клиент вызывает `/predict` с тем же форматом запроса
   - Тогда: формат ответа не ломается (либо изменения документированы и покрыты тестами)

---

## 8. Как это всё применять по минимуму в курсовом проекте

### 8.1. Базовый уровень (желательно для всех)

- Несколько unit-тестов на функции предобработки данных.
- Хотя бы один тест на `/health` и один на `/predict` (успешный кейс).

### 8.2. Продвинутый уровень (для тех, кто хочет «+ качество»)

- Небольшой набор BDD-сценариев для API (успех + ошибки).
- Тесты, проверяющие контракт данных:

  - колонки не исчезли/не переименовались;
  - типы полей соответствуют ожиданиям.

### 8.3. Очень хорошо, если

- тесты запускаются одной командой (`pytest`);
- тесты входят в CI или хотя бы всегда прогоняются локально перед коммитом.

---

## 9. Мини-чеклист по TDD/BDD для своего проекта

1. Есть ли у тебя **хотя бы 3–5 unit-тестов** на ключевые функции (предобработка, полезные утилиты)?
2. Есть ли **тест на `/predict`**, который:

   - запускает сервис локально (или через TestClient),
   - проверяет статус и формат ответа?
3. Есть ли хотя бы один тест/сценарий, который:

   - проверяет обработку некорректных/частично некорректных данных?
4. Понимаешь ли ты словами **Given–When–Then**, что делает твой сервис?
5. Прогоняешь ли ты тесты перед крупными изменениями (или хотя бы перед сдачей проекта)?

Если на большинство пунктов можно ответить «да», то твой проект находится на вполне приличном уровне инженерной дисциплины для курса.

---
