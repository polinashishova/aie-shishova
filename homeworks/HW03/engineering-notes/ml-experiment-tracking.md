# Трекинг ML-экспериментов

Эта заметка про:

- зачем вообще отслеживать эксперименты в ML/ИИ-проекте;
- минимальный уровень для курсового проекта (имена, заметки, табличка);
- как аккуратно хранить метрики и артефакты;
- куда можно вырасти (MLflow и аналоги).

---

## 1. Зачем нужен трекинг экспериментов

Без трекинга получается классика:

- «эта модель вроде лучше, но я не помню, с какими параметрами её обучал»;
- «метрика в отчёте не совпадает с тем, что вижу сейчас в ноутбуке»;
- «случайно переписал файл `model.pkl`, а старую версию уже не восстановить».

Эксперимент в ML/ИИ — это не только «запустил обучение», а связка:

- data: какие данные и в каком виде использовали;
- code: какая версия кода/модели;
- params: гиперпараметры, конфигурация пайплайна;
- metrics: результаты (метрики, графики);
- artifacts: сохранённые модели, фичи, лог-файлы.

Трекинг нужен, чтобы:

- сравнивать эксперименты честно;
- воспроизводить удачные версии;
- спокойно писать отчёт и объяснять, что делал.

---

## 2. Минимальный уровень для курсового проекта

Для курса **не обязательно** поднимать полноценную MLOps-платформу.  
Достаточно дисциплины и пары простых инструментов.

### 2.1. Где жить экспериментам

Рекомендуемая структура (внутри `project/`):

```text
project/
  experiments/
    README.md
    runs/
      exp_001_baseline/
      exp_002_more_trees/
      exp_003_new_features/
  models/
  data/
  notebooks/
  src/
  configs/
```

- `experiments/runs/exp_XXX_name/` — папка для конкретного запуска/набора запусков.
- Внутри каждой `exp_XXX_*` можно хранить:

  - конфиг (копию `yaml/json` с параметрами);
  - краткий лог/описание в Markdown;
  - метрики в виде маленького `.json`/`.csv`;
  - ссылку на модель/артефакты.

### 2.2. Именование экспериментов

Базовый паттерн:

```text
exp_001_baseline
exp_002_more_trees
exp_003_new_features
exp_004_tuned_lr
```

Где:

- `001`, `002`, … — порядковый номер;
- короткое имя — о **главной идее** эксперимента.

Плохие имена:

- `exp_1`, `exp_2`, `final`, `new_model`.

Хорошие:

- `exp_005_add_age_bucket_feature`;
- `exp_006_drop_rare_categories`;
- `exp_010_resnet50_pretrained`.

---

## 3. Простая табличка экспериментов

Даже простая табличка сильно помогает. Можно:

- `project/experiments/experiments.csv` или
- `project/experiments/experiments.md` (таблица в Markdown),
- или просто `experiments.xlsx`.

Пример структуры (словами, без жёсткого формата):

- `id` — `exp_001`, `exp_002`…
- `description` — что изменилось;
- `data_version` — какая версия данных (см. `data-versioning.md`);
- `config` — какой файл конфига использовали;
- `model_path` — путь к сохранённой модели;
- `metrics` — ключевые метрики (AUC, accuracy, RMSE и т.д.);
- `comment` — короткий вывод («лучше/хуже baseline, почему»).

Главная идея:

> Открывая эту табличку через месяц, ты должен понимать, что это за эксперимент, чем он отличается от соседних и почему он важен.

---

## 4. Мини-лог эксперимента

Для каждого `exp_XXX_*` полезно иметь мини-лог (Markdown):

`project/experiments/runs/exp_003_new_features/README.md`:

```markdown
# Эксперимент exp_003_new_features

Дата: 2025-10-10  
Цель: проверить влияние новых фич (age_bucket, is_active_30d) на качество модели.

## Настройки

- Данные: data/processed/train_v2.csv
- Конфиг: configs/experiment_v2.yaml
- Модель: RandomForestClassifier
  - n_estimators: 300
  - max_depth: 8
  - random_state: 42
- Сплит: train/val (80/20, stratifed)

## Результаты

- ROC-AUC (val): 0.842
- ROC-AUC (test): 0.835
- Accuracy (val): 0.78

## Выводы

- По сравнению с exp_002 (без новых фич) ROC-AUC вырос с 0.81 до 0.84.
- Фичи age_bucket и is_active_30d дают заметный прирост качества.
- Имеет смысл оставить их в дальнейших экспериментах.
```

Это уже полноценный «экспериментальный журнал».

---

## 5. Метрики и артефакты: как сохранять

### 5.1. Метрики

Внутри `exp_XXX_*` можно хранить:

- `metrics.json`:

```json
{
  "roc_auc_val": 0.842,
  "roc_auc_test": 0.835,
  "accuracy_val": 0.78,
  "train_time_sec": 45.3,
  "n_params": 123456
}
```

или

- `metrics.csv` (если хочется потом сводить в таблицы):

```text
metric_name,metric_value
roc_auc_val,0.842
roc_auc_test,0.835
accuracy_val,0.78
train_time_sec,45.3
n_params,123456
```

Главное — чтобы у тебя был **машиночитаемый** файл, по которому можно построить сводку.

### 5.2. Модели и другие артефакты

Сами модели лучше хранить отдельно:

- `project/models/checkpoints/model_exp_003.pkl`;
- `project/models/final/model_v2.pkl`.

В `exp_XXX_*` достаточно положить ссылку/путь, чтобы понимать, чем связан эксперимент и модель.

Если используешь Git LFS — можно подключить его для крупных файлов весов.

---

## 6. От ноутбука к трекингу

Типичная эволюция:

1. Сначала всё в одном ноутбуке: параметр поменял, ячейку перегнал, метрику посмотрел, забыл.
2. Потом появляется таблица в конце ноутбука: `experiment`, `params`, `metrics`.
3. Чуть позже:

   - выносишь логику обучения в `src/`;
   - делаешь скрипт обучения, который:

     - принимает конфиг;
     - по окончании записывает `metrics.json` и кладёт модель в нужную папку;
     - пишет в лог минимальную информацию.

Мини-скрипт с логированием в JSON (упрощённо):

```python
import json
from pathlib import Path
from datetime import datetime

def save_metrics(metrics: dict, exp_dir: Path) -> None:
    exp_dir.mkdir(parents=True, exist_ok=True)
    metrics["timestamp"] = datetime.utcnow().isoformat()
    with open(exp_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)
```

Так код и трекинг начинают дружить.

---

## 7. Автоматизированные трекеры: MLflow и аналоги

В «взрослых» проектах часто используются инструменты:

- **MLflow Tracking**;
- Weights & Biases (wandb);
- Neptune, Comet и т.д.

Общее:

- один или несколько серверов/сервисов для логирования;
- Python SDK:

  - `mlflow.log_metric("roc_auc", value)`;
  - `mlflow.log_artifact("model.pkl")`;
- UI, где можно видеть:

  - список экспериментов;
  - параметры;
  - метрики;
  - артефакты;
  - сравнивать эксперименты между собой.

Для курсового проекта:

- использовать MLflow **не обязательно**, но можно как «плюс в инженерности»;
- важно, чтобы при этом:

  - структура проекта оставалась читабельной;
  - базовая часть (код, данные, модели, отчёт) была понятна без UI MLflow.

---

## 8. Как трекинг помогает писать отчёт

Хороший `report.md` по проекту обычно содержит:

- раздел «Экспериментальный дизайн»:

  - какие эксперименты проводились;
  - какие гипотезы проверялись;
- таблицу/перечень ключевых экспериментов с:

  - id;
  - параметрами (кратко);
  - метриками;
  - выводом.

Если у тебя есть:

- структурированная папка `experiments/`;
- табличка/метрики по экспериментам;

то отчёт пишется:

- не из памяти «кажется, там было 0.83»;
- а по фактическим данным.

---

## 9. Мини-чеклист по эксперимент-трекингу для проекта

Перед защитой проекта задай себе вопросы:

1. Есть ли у меня в `project/` **явное место** для экспериментов (`experiments/` или аналог)?
2. Могу ли я назвать **3–5 экспериментов** и показать:

   - чем они отличаются (данные/модель/параметры);
   - какие метрики у каждого;
   - почему финальный выбор сделан в пользу конкретного?
3. Есть ли текстовый/табличный **обзор экспериментов** (в `experiments/` или `report.md`)?
4. Понимаю ли я, **какая модель** и с **какого эксперимента** используется сейчас в `/predict`?
5. Если преподаватель спросит:

   - «почему эта модель, а не та?»
   - «чем эта версия данных отличается от предыдущей?» —
     смогу ли я ответить, опираясь на свои записи?

Если большая часть ответов — «да», то твой трекинг экспериментов на очень хорошем уровне для курсового проекта по инженерии ИИ. Если нет — начать можно с самого простого: папка `experiments/` + табличка + короткие README к ключевым экспериментам.

---
